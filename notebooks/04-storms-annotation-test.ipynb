{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import climdist.ner.doccano_transformations as dt\n",
    "from spacy.util import minibatch, compounding \n",
    "from spacy.training import Example\n",
    "from spacy import displacy\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les données saisies à la main depuis un fichier Excel.\n",
    "# il y a surtout des tempêtes, mais aussi un certain nombre des autres phénomènes\n",
    "df = pd.read_excel(r'storms_for_spacy.xlsx')\n",
    "\n",
    "# supprimer les colonnes vides\n",
    "df = df[['YEAR', 'MONTH', 'DATE_BEGIN', 'DATE_END', 'CAT_ID', 'COMP_ID','LOD_ID', 'EXC', 'COMMENT', 'LINK']]\n",
    "\n",
    "# certaines entrées n'ont pas de l'extrait de la source - les supprimer\n",
    "df = df.loc[pd.notna(df['EXC'])]\n",
    "\n",
    "# créer une colonne qui mésure la longueur des extraits (len(str))\n",
    "df['len'] = df['EXC'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer un nouvel dataframe qui ne contient que les entrées dont la longueur de l'extrait dépasse une longueur donnée\n",
    "# (j'ai choisi 100 charactères pour donner un peu plus de contexte et éliminer les descriptions très laconiques d'un point de vue NLP)\n",
    "# cependant, la limite est tout à faite arbitraire à ce moment\n",
    "shortdf = df[df['len'] > 99]\n",
    "\n",
    "# mettre en place un ordre chronologique pour le dataframe\n",
    "shortdf = shortdf.sort_values(['YEAR', 'MONTH', 'DATE_BEGIN'])\n",
    "\n",
    "#shortdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# écrire tous les extraits sélectionnés dans un fichier .txt\n",
    "\n",
    "with open('storms_spacy.txt', 'w', encoding = 'utf8') as myfile:\n",
    "    for txt in shortdf['EXC']:\n",
    "        storm = txt + '\\n\\n'\n",
    "        myfile.write(storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j'ai décidé d'experimenter avec Spacy, mais il reste à trouver si il y a des modèles pour l'allemand historique, p. ex. en NLTK\n",
    "nlp = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('storms_spacy.txt', 'r', encoding='utf8') as myfile:\n",
    "    data = myfile.read()\n",
    "    \n",
    "doc = nlp(data)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# une petite exploration des entités nommées. étonnamment, Spacy est assez performant pour la catégorie LOC\n",
    "# en plus, il reconnait un certain nombre des phénomènes météorologiques (voir #100, #103, #108-110, #112, #119, #146 etc),\n",
    "# mais en les étiquetant comme des ORG, LOC ou PER.\n",
    "\n",
    "ents_dico = {'text':[ent.text for ent in doc.ents], 'label':[ent.label_ for ent in doc.ents]}\n",
    "\n",
    "ents_df = pd.DataFrame(data = ents_dico, columns = ['text', 'label'])\n",
    "ents_df[100:115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dt.doccano_to_spacy('../pipeline/03_ner/01_doccano/storms_annotated_19.03v2.jsonl')\n",
    "train_data = dt.doccano_strip(train_data)\n",
    "train_data = dt.doccano_strip(train_data)\n",
    "train_data = dt.wea_to_nat(train_data)\n",
    "#print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "for annotations in train_data:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "        ner.add_label(ent[2])\n",
    "        \n",
    "ner.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(20):\n",
    "        print(\"iteration: \"+str(itn))\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            for ba in batch:\n",
    "                examples.append(Example.from_dict(nlp.make_doc(ba[\"text\"]), ba))\n",
    "                nlp.update(examples)        \n",
    "print(\"training is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"storms_ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading from\", output_dir)\n",
    "nlp_test = spacy.load(output_dir)\n",
    "print(\"Loading finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy_color_code = {'WEA': '#4cafd9',\n",
    "                  'PER': '#ffb366',\n",
    "                  'DAT': '#bf80ff',\n",
    "                  'LOC': '#a88676',\n",
    "                  'MISC': 'grey',\n",
    "                  'MEA': '#85e085',\n",
    "                  'ORG': '#5353c6'}\n",
    "\n",
    "displacy_options = {'ents': ['WEA', 'PER', 'DAT', 'LOC', 'MISC', 'MEA', 'ORG'], 'colors': displacy_color_code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_test_generator(filepath):\n",
    "    \n",
    "    for file in glob.glob(filepath):\n",
    "        docname = path.splitext(path.basename(file))[0]\n",
    "        print(docname)\n",
    "        with open(file, 'r', encoding='utf8') as f:\n",
    "            doctext = f.read()\n",
    "            doctext = doctext.replace('\\n', ' ')\n",
    "        \n",
    "        yield docname, nlp_test(doctext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_tests_gen = nlp_test_generator('./test_data/TP_tests/*.txt')\n",
    "\n",
    "tp_tests = [docfile for docname, docfile in tp_tests_gen]\n",
    "\n",
    "html = displacy.render(tp_tests, style='ent', page=True, jupyter=False, options=displacy_options)\n",
    "with open('./test_data/TP_tests/tp_tests.html', 'w', encoding='utf8') as f:\n",
    "    f.write(html)\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_tests_gen = nlp_test_generator('./test_data/FP_tests/*.txt')\n",
    "\n",
    "fp_tests = [docfile for docname, docfile in fp_tests_gen]\n",
    "\n",
    "html = displacy.render(fp_tests, style='ent', page=True, jupyter=False, options=displacy_options)\n",
    "with open('./test_data/FP_tests/fp_tests.html', 'w', encoding='utf8') as f:\n",
    "    f.write(html)\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test_data/FP_tests/LZ_Nr002_1862.txt', 'r', encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "    data = data.replace('\\n', ' ')\n",
    "    doc_lz = nlp_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc_lz, style='ent', jupyter = True, options = displacy_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp_test('Hamburg, 3. Jan. Als Vorläuferin der Unglücksbot ſchaften, welche wir von der See her nach dem letzten Sturm zu erwarten haben, iſt die heute eingetroffene Nachricht, daß das Packetſchiff „George Canning“, Hrn. R. M. Slomann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc2, style='ent', jupyter = True, options = displacy_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"overwrite_ents\": True }\n",
    "ruler = nlp.add_pipe('entity_ruler', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_phenomena = [\"Sturm\", \"Hagel\"]\n",
    "for n in natural_phenomena:\n",
    "    ruler.add_patterns([{\"label\": \"NAT\", \"pattern\": n}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp('Heute haben wir einen starken Sturm un vielen Hagel gehabt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
