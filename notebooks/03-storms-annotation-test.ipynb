{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import climdist.ner.doccano_transformations as dt\n",
    "from spacy.util import minibatch, compounding \n",
    "from spacy.training import Example\n",
    "from spacy import displacy\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DATE_BEGIN</th>\n",
       "      <th>DATE_END</th>\n",
       "      <th>CAT_ID</th>\n",
       "      <th>COMP_ID</th>\n",
       "      <th>LOD_ID</th>\n",
       "      <th>EXC</th>\n",
       "      <th>COMMENT</th>\n",
       "      <th>LINK</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1881</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1:1:5:4:, 1:1:4:2:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lv:ml:riga:</td>\n",
       "      <td>Am Vor- und Nachmittag Regen und Sturm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://periodika.lv/periodika2-viewer/view/ind...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1812</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>1:1:5:4:, 1:1:4:5:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lv:ml:riga:</td>\n",
       "      <td>Sturm aus N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://periodika.lv/periodika2-viewer/view/ind...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1855</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1:1:5:4:, 1:1:4:2:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ee:ne:tallinn:</td>\n",
       "      <td>Bei gelinder Witterung liefen im vorigen Monat...</td>\n",
       "      <td>Tallinna kirjasaatja 2. jaanuaril</td>\n",
       "      <td>http://periodika.lv/periodika2-viewer/view/ind...</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1891</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>1:1:5:4:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lv:wl:riga:</td>\n",
       "      <td>Die stürmische Witterung, welche am Morgen des...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://periodika.lv/periodika2-viewer/view/ind...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1891</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>1:1:5:4:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lv:wl:vp:pope:</td>\n",
       "      <td>Aus Popen (Kurland) wird uns geschrieben, daß ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://periodika.lv/periodika2-viewer/view/ind...</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH DATE_BEGIN DATE_END              CAT_ID COMP_ID  \\\n",
       "0  1881   12.0         30       30  1:1:5:4:, 1:1:4:2:     NaN   \n",
       "2  1812   12.0         18       18  1:1:5:4:, 1:1:4:5:     NaN   \n",
       "3  1855   12.0         22      NaN  1:1:5:4:, 1:1:4:2:     NaN   \n",
       "4  1891   12.0         25       27            1:1:5:4:     NaN   \n",
       "5  1891   12.0         25       26            1:1:5:4:     NaN   \n",
       "\n",
       "           LOD_ID                                                EXC  \\\n",
       "0     lv:ml:riga:             Am Vor- und Nachmittag Regen und Sturm   \n",
       "2     lv:ml:riga:                                        Sturm aus N   \n",
       "3  ee:ne:tallinn:  Bei gelinder Witterung liefen im vorigen Monat...   \n",
       "4     lv:wl:riga:  Die stürmische Witterung, welche am Morgen des...   \n",
       "5  lv:wl:vp:pope:  Aus Popen (Kurland) wird uns geschrieben, daß ...   \n",
       "\n",
       "                             COMMENT  \\\n",
       "0                                NaN   \n",
       "2                                NaN   \n",
       "3  Tallinna kirjasaatja 2. jaanuaril   \n",
       "4                                NaN   \n",
       "5                                NaN   \n",
       "\n",
       "                                                LINK  len  \n",
       "0  http://periodika.lv/periodika2-viewer/view/ind...   38  \n",
       "2  http://periodika.lv/periodika2-viewer/view/ind...   11  \n",
       "3  http://periodika.lv/periodika2-viewer/view/ind...  395  \n",
       "4  http://periodika.lv/periodika2-viewer/view/ind...  149  \n",
       "5  http://periodika.lv/periodika2-viewer/view/ind...  219  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importer les données saisies à la main depuis un fichier Excel.\n",
    "# il y a surtout des tempêtes, mais aussi un certain nombre des autres phénomènes\n",
    "df = pd.read_excel('../data/processed/storms_for_annotation.xlsx')\n",
    "\n",
    "# supprimer les colonnes vides\n",
    "df = df[['YEAR', 'MONTH', 'DATE_BEGIN', 'DATE_END', 'CAT_ID', 'COMP_ID','LOD_ID', 'EXC', 'COMMENT', 'LINK']]\n",
    "\n",
    "# certaines entrées n'ont pas de l'extrait de la source - les supprimer\n",
    "df = df.loc[pd.notna(df['EXC'])]\n",
    "\n",
    "# créer une colonne qui mésure la longueur des extraits (len(str))\n",
    "df['len'] = df['EXC'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer un nouvel dataframe qui ne contient que les entrées dont la longueur de l'extrait dépasse une longueur donnée\n",
    "# (j'ai choisi 100 charactères pour donner un peu plus de contexte et éliminer les descriptions très laconiques d'un point de vue NLP)\n",
    "# cependant, la limite est tout à faite arbitraire à ce moment\n",
    "shortdf = df[df['len'] > 99]\n",
    "\n",
    "# mettre en place un ordre chronologique pour le dataframe\n",
    "shortdf = shortdf.sort_values(['YEAR', 'MONTH', 'DATE_BEGIN'])\n",
    "\n",
    "#shortdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>567.000000</td>\n",
       "      <td>562.000000</td>\n",
       "      <td>567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1866.774250</td>\n",
       "      <td>7.208185</td>\n",
       "      <td>242.645503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>27.515852</td>\n",
       "      <td>3.350218</td>\n",
       "      <td>166.816967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1603.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1847.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>142.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1880.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1885.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>275.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1899.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1525.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              YEAR       MONTH          len\n",
       "count   567.000000  562.000000   567.000000\n",
       "mean   1866.774250    7.208185   242.645503\n",
       "std      27.515852    3.350218   166.816967\n",
       "min    1603.000000    1.000000   100.000000\n",
       "25%    1847.000000    4.000000   142.000000\n",
       "50%    1880.000000    8.000000   191.000000\n",
       "75%    1885.000000   10.000000   275.500000\n",
       "max    1899.000000   12.000000  1525.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# écrire tous les extraits sélectionnés dans un fichier .txt\n",
    "\n",
    "with open('storms_spacy.txt', 'w', encoding = 'utf8') as myfile:\n",
    "    for txt in shortdf['EXC']:\n",
    "        storm = txt + '\\n\\n'\n",
    "        myfile.write(storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j'ai décidé d'experimenter avec Spacy, mais il reste à trouver si il y a des modèles pour l'allemand historique, p. ex. en NLTK\n",
    "nlp = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('storms_spacy.txt', 'r', encoding='utf8') as myfile:\n",
    "    data = myfile.read()\n",
    "    \n",
    "doc = nlp(data)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# une petite exploration des entités nommées. étonnamment, Spacy est assez performant pour la catégorie LOC\n",
    "# en plus, il reconnait un certain nombre des phénomènes météorologiques (voir #100, #103, #108-110, #112, #119, #146 etc),\n",
    "# mais en les étiquetant comme des ORG, LOC ou PER.\n",
    "\n",
    "ents_dico = {'text':[ent.text for ent in doc.ents], 'label':[ent.label_ for ent in doc.ents]}\n",
    "\n",
    "ents_df = pd.DataFrame(data = ents_dico, columns = ['text', 'label'])\n",
    "ents_df[100:115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dt.doccano_to_spacy('../pipeline/03_ner/01_doccano/storms_annotated_19.03v2.jsonl')\n",
    "train_data = dt.doccano_strip(train_data)\n",
    "train_data = dt.doccano_strip(train_data)\n",
    "train_data = dt.wea_to_nat(train_data)\n",
    "#print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "for annotations in train_data:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "        ner.add_label(ent[2])\n",
    "        \n",
    "ner.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(20):\n",
    "        print(\"iteration: \"+str(itn))\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            for ba in batch:\n",
    "                examples.append(Example.from_dict(nlp.make_doc(ba[\"text\"]), ba))\n",
    "                nlp.update(examples)        \n",
    "print(\"training is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"storms_ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading from\", output_dir)\n",
    "nlp_test = spacy.load(output_dir)\n",
    "print(\"Loading finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy_color_code = {'WEA': '#4cafd9',\n",
    "                  'PER': '#ffb366',\n",
    "                  'DAT': '#bf80ff',\n",
    "                  'LOC': '#a88676',\n",
    "                  'MISC': 'grey',\n",
    "                  'MEA': '#85e085',\n",
    "                  'ORG': '#5353c6'}\n",
    "\n",
    "displacy_options = {'ents': ['WEA', 'PER', 'DAT', 'LOC', 'MISC', 'MEA', 'ORG'], 'colors': displacy_color_code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_test_generator(filepath):\n",
    "    \n",
    "    for file in glob.glob(filepath):\n",
    "        docname = path.splitext(path.basename(file))[0]\n",
    "        print(docname)\n",
    "        with open(file, 'r', encoding='utf8') as f:\n",
    "            doctext = f.read()\n",
    "            doctext = doctext.replace('\\n', ' ')\n",
    "        \n",
    "        yield docname, nlp_test(doctext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_tests_gen = nlp_test_generator('./test_data/TP_tests/*.txt')\n",
    "\n",
    "tp_tests = [docfile for docname, docfile in tp_tests_gen]\n",
    "\n",
    "html = displacy.render(tp_tests, style='ent', page=True, jupyter=False, options=displacy_options)\n",
    "with open('./test_data/TP_tests/tp_tests.html', 'w', encoding='utf8') as f:\n",
    "    f.write(html)\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_tests_gen = nlp_test_generator('./test_data/FP_tests/*.txt')\n",
    "\n",
    "fp_tests = [docfile for docname, docfile in fp_tests_gen]\n",
    "\n",
    "html = displacy.render(fp_tests, style='ent', page=True, jupyter=False, options=displacy_options)\n",
    "with open('./test_data/FP_tests/fp_tests.html', 'w', encoding='utf8') as f:\n",
    "    f.write(html)\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test_data/FP_tests/LZ_Nr002_1862.txt', 'r', encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "    data = data.replace('\\n', ' ')\n",
    "    doc_lz = nlp_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc_lz, style='ent', jupyter = True, options = displacy_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp_test('Hamburg, 3. Jan. Als Vorläuferin der Unglücksbot ſchaften, welche wir von der See her nach dem letzten Sturm zu erwarten haben, iſt die heute eingetroffene Nachricht, daß das Packetſchiff „George Canning“, Hrn. R. M. Slomann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc2, style='ent', jupyter = True, options = displacy_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"overwrite_ents\": True }\n",
    "ruler = nlp.add_pipe('entity_ruler', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_phenomena = [\"Sturm\", \"Hagel\"]\n",
    "for n in natural_phenomena:\n",
    "    ruler.add_patterns([{\"label\": \"NAT\", \"pattern\": n}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp('Heute haben wir einen starken Sturm un vielen Hagel gehabt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
