{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "occupational-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import climdist.data\n",
    "import json\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from climdist.doccano import Transformer\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "progressive-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cross-rainbow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>pub</th>\n",
       "      <th>heading</th>\n",
       "      <th>full_text</th>\n",
       "      <th>href</th>\n",
       "      <th>text_len</th>\n",
       "      <th>readability</th>\n",
       "      <th>heading2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1802-01-01</td>\n",
       "      <td>1802</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Rigasche Zeitung</td>\n",
       "      <td>St. Petersburg, den 20. Decemb.</td>\n",
       "      <td>St. Petersburg, den 20. Decemb.\\n\\n\\tAuf Aller...</td>\n",
       "      <td>https://proc.dom.lndb.lv/file.axd?id=3377815&amp;a...</td>\n",
       "      <td>2067</td>\n",
       "      <td>1</td>\n",
       "      <td>St. Petersburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1802-01-01</td>\n",
       "      <td>1802</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Rigasche Zeitung</td>\n",
       "      <td>Paris, den 18ten December.</td>\n",
       "      <td>Paris, den 18ten December.\\n\\n\\tDer Oberconsu!...</td>\n",
       "      <td>https://proc.dom.lndb.lv/file.axd?id=3377818&amp;a...</td>\n",
       "      <td>3664</td>\n",
       "      <td>1</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1802-01-01</td>\n",
       "      <td>1802</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Rigasche Zeitung</td>\n",
       "      <td>Haag, den 22sten December.</td>\n",
       "      <td>Haag, den 22sten December.\\n\\n\\tIn kurzem erwa...</td>\n",
       "      <td>https://proc.dom.lndb.lv/file.axd?id=3377819&amp;a...</td>\n",
       "      <td>967</td>\n",
       "      <td>1</td>\n",
       "      <td>Haag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1802-01-01</td>\n",
       "      <td>1802</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Rigasche Zeitung</td>\n",
       "      <td>Zürich, den 16ten December.</td>\n",
       "      <td>Zürich, den 16ten December.\\n\\n\\tDie durch die...</td>\n",
       "      <td>https://proc.dom.lndb.lv/file.axd?id=3377822&amp;a...</td>\n",
       "      <td>482</td>\n",
       "      <td>1</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1802-01-01</td>\n",
       "      <td>1802</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Rigasche Zeitung</td>\n",
       "      <td>Wien, den 19ten December.</td>\n",
       "      <td>Wien, den 19ten December.\\n\\n\\tDie Malcheser-N...</td>\n",
       "      <td>https://proc.dom.lndb.lv/file.axd?id=3377823&amp;a...</td>\n",
       "      <td>1154</td>\n",
       "      <td>1</td>\n",
       "      <td>Wien</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  year  month  day               pub  \\\n",
       "0  1802-01-01  1802      1    1  Rigasche Zeitung   \n",
       "1  1802-01-01  1802      1    1  Rigasche Zeitung   \n",
       "2  1802-01-01  1802      1    1  Rigasche Zeitung   \n",
       "3  1802-01-01  1802      1    1  Rigasche Zeitung   \n",
       "4  1802-01-01  1802      1    1  Rigasche Zeitung   \n",
       "\n",
       "                           heading  \\\n",
       "0  St. Petersburg, den 20. Decemb.   \n",
       "1       Paris, den 18ten December.   \n",
       "2       Haag, den 22sten December.   \n",
       "3      Zürich, den 16ten December.   \n",
       "4        Wien, den 19ten December.   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  St. Petersburg, den 20. Decemb.\\n\\n\\tAuf Aller...   \n",
       "1  Paris, den 18ten December.\\n\\n\\tDer Oberconsu!...   \n",
       "2  Haag, den 22sten December.\\n\\n\\tIn kurzem erwa...   \n",
       "3  Zürich, den 16ten December.\\n\\n\\tDie durch die...   \n",
       "4  Wien, den 19ten December.\\n\\n\\tDie Malcheser-N...   \n",
       "\n",
       "                                                href  text_len  readability  \\\n",
       "0  https://proc.dom.lndb.lv/file.axd?id=3377815&a...      2067            1   \n",
       "1  https://proc.dom.lndb.lv/file.axd?id=3377818&a...      3664            1   \n",
       "2  https://proc.dom.lndb.lv/file.axd?id=3377819&a...       967            1   \n",
       "3  https://proc.dom.lndb.lv/file.axd?id=3377822&a...       482            1   \n",
       "4  https://proc.dom.lndb.lv/file.axd?id=3377823&a...      1154            1   \n",
       "\n",
       "         heading2  \n",
       "0  St. Petersburg  \n",
       "1           Paris  \n",
       "2            Haag  \n",
       "3          Zürich  \n",
       "4            Wien  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "formal-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = ['Regen',\n",
    "          'Regens',\n",
    "          'Regenfall',\n",
    "          'Niederschlag',\n",
    "          'Niederschlages',\n",
    "          'Guss',\n",
    "          'Regenschauer',\n",
    "          'Regenguss',\n",
    "          'Wolkenbruch',\n",
    "          'Regenguß',\n",
    "          'Schnee',\n",
    "          'Schneefall',\n",
    "          'Niederschlag',\n",
    "          'Schneebruch',\n",
    "          'Dürre',\n",
    "          'Dürres',\n",
    "          'Graupel',\n",
    "          'Hagel',\n",
    "          'Hagels',\n",
    "          'Hagelschlag',\n",
    "          'Gewitter',\n",
    "          'Gewitters',\n",
    "          'Tau',\n",
    "          'Thau',\n",
    "          'Tauwetter',\n",
    "          'Thauwetter',\n",
    "          'Nebel',\n",
    "          'Nebels']\n",
    "\n",
    "wind = ['Wind',\n",
    "        'Winde',\n",
    "        'Windes',\n",
    "        'Winden',\n",
    "        'Sturm',\n",
    "        'Stürme',\n",
    "        'Stürmen',\n",
    "        'Sturmes',\n",
    "        'Sturmwind',\n",
    "        'Tornado',\n",
    "        'Orkan',\n",
    "        'Wirbelwind',\n",
    "        'Wirbelsturm',\n",
    "        'Unwetter',\n",
    "        'Taifun',\n",
    "        'Südwind',\n",
    "        'Südwestwind',\n",
    "        'Westwind',\n",
    "        'Nordwestwind',\n",
    "        'Nordwind',\n",
    "        'Nordostwind',\n",
    "        'Ostwind',\n",
    "        'Südostwind',\n",
    "        'S. Wind',\n",
    "        'S.W. Wind',\n",
    "        'S. W. Wind',\n",
    "        'W. Wind',\n",
    "        'N.W. Wind',\n",
    "        'N. W. Wind',\n",
    "        'N. Wind',\n",
    "        'N.O. Wind',\n",
    "        'N. O. Wind',\n",
    "        'O. Wind',\n",
    "        'S.O. Wind',\n",
    "        'S. O. Wind']\n",
    "\n",
    "flood = ['Flut',\n",
    "         'Sutrmflut',\n",
    "         'Hochwasser',\n",
    "         'Hochflut',\n",
    "         'Überschwemmung',\n",
    "         'Überschweummungs',\n",
    "         'Überschwemmungen',\n",
    "         'Überflutung',\n",
    "         'Überflutungen']\n",
    "\n",
    "temperature = ['Kälte',\n",
    "               'Kältes',\n",
    "               'Wärme',\n",
    "               'Wärmes',\n",
    "               'Hitze',\n",
    "               'Hitzewelle',\n",
    "               'Kältewelle',\n",
    "               'Frost']\n",
    "\n",
    "general = ['Wetter',\n",
    "           'Wetters',\n",
    "           'Witterung',\n",
    "           'Witterungs',\n",
    "           'Klima',\n",
    "           'Klimas',\n",
    "           'Klimaereignis',\n",
    "           'Wetterereignis',\n",
    "           'Windstärke',\n",
    "           'Luftdruck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "attractive-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_events = {'general': general,\n",
    "                  'temperature': temperature,\n",
    "                  'precipitations': precip,\n",
    "                  'wind': wind,\n",
    "                  'flood': flood}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "heated-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the weather events for entity ruler\n",
    "\n",
    "with open('../pipeline/weather_events.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(weather_events, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "limited-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create patterns for ruler (assign label by surface text)\n",
    "\n",
    "patterns = []\n",
    "\n",
    "for category in weather_events.values():\n",
    "    for phrase in category:\n",
    "        if ' ' in phrase:\n",
    "            patterns.append({\n",
    "                'label': 'WEA',\n",
    "                'pattern': [{'LOWER': piece} for piece in phrase.split(' ')]\n",
    "            })\n",
    "        else:\n",
    "            patterns.append({'label': 'WEA', 'pattern': phrase})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "respiratory-nancy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'WEA', 'pattern': 'Wetter'},\n",
       " {'label': 'WEA', 'pattern': 'Wetters'},\n",
       " {'label': 'WEA', 'pattern': 'Witterung'},\n",
       " {'label': 'WEA', 'pattern': 'Witterungs'},\n",
       " {'label': 'WEA', 'pattern': 'Klima'},\n",
       " {'label': 'WEA', 'pattern': 'Klimas'},\n",
       " {'label': 'WEA', 'pattern': 'Klimaereignis'},\n",
       " {'label': 'WEA', 'pattern': 'Wetterereignis'},\n",
       " {'label': 'WEA', 'pattern': 'Windstärke'},\n",
       " {'label': 'WEA', 'pattern': 'Luftdruck'},\n",
       " {'label': 'WEA', 'pattern': 'Kälte'},\n",
       " {'label': 'WEA', 'pattern': 'Kältes'},\n",
       " {'label': 'WEA', 'pattern': 'Wärme'},\n",
       " {'label': 'WEA', 'pattern': 'Wärmes'},\n",
       " {'label': 'WEA', 'pattern': 'Hitze'},\n",
       " {'label': 'WEA', 'pattern': 'Hitzewelle'},\n",
       " {'label': 'WEA', 'pattern': 'Kältewelle'},\n",
       " {'label': 'WEA', 'pattern': 'Frost'},\n",
       " {'label': 'WEA', 'pattern': 'Regen'},\n",
       " {'label': 'WEA', 'pattern': 'Regens'},\n",
       " {'label': 'WEA', 'pattern': 'Regenfall'},\n",
       " {'label': 'WEA', 'pattern': 'Niederschlag'},\n",
       " {'label': 'WEA', 'pattern': 'Niederschlages'},\n",
       " {'label': 'WEA', 'pattern': 'Guss'},\n",
       " {'label': 'WEA', 'pattern': 'Regenschauer'},\n",
       " {'label': 'WEA', 'pattern': 'Regenguss'},\n",
       " {'label': 'WEA', 'pattern': 'Wolkenbruch'},\n",
       " {'label': 'WEA', 'pattern': 'Regenguß'},\n",
       " {'label': 'WEA', 'pattern': 'Schnee'},\n",
       " {'label': 'WEA', 'pattern': 'Schneefall'},\n",
       " {'label': 'WEA', 'pattern': 'Niederschlag'},\n",
       " {'label': 'WEA', 'pattern': 'Schneebruch'},\n",
       " {'label': 'WEA', 'pattern': 'Dürre'},\n",
       " {'label': 'WEA', 'pattern': 'Dürres'},\n",
       " {'label': 'WEA', 'pattern': 'Graupel'},\n",
       " {'label': 'WEA', 'pattern': 'Hagel'},\n",
       " {'label': 'WEA', 'pattern': 'Hagels'},\n",
       " {'label': 'WEA', 'pattern': 'Hagelschlag'},\n",
       " {'label': 'WEA', 'pattern': 'Gewitter'},\n",
       " {'label': 'WEA', 'pattern': 'Gewitters'},\n",
       " {'label': 'WEA', 'pattern': 'Tau'},\n",
       " {'label': 'WEA', 'pattern': 'Thau'},\n",
       " {'label': 'WEA', 'pattern': 'Tauwetter'},\n",
       " {'label': 'WEA', 'pattern': 'Thauwetter'},\n",
       " {'label': 'WEA', 'pattern': 'Nebel'},\n",
       " {'label': 'WEA', 'pattern': 'Nebels'},\n",
       " {'label': 'WEA', 'pattern': 'Wind'},\n",
       " {'label': 'WEA', 'pattern': 'Winde'},\n",
       " {'label': 'WEA', 'pattern': 'Windes'},\n",
       " {'label': 'WEA', 'pattern': 'Winden'},\n",
       " {'label': 'WEA', 'pattern': 'Sturm'},\n",
       " {'label': 'WEA', 'pattern': 'Stürme'},\n",
       " {'label': 'WEA', 'pattern': 'Stürmen'},\n",
       " {'label': 'WEA', 'pattern': 'Sturmes'},\n",
       " {'label': 'WEA', 'pattern': 'Sturmwind'},\n",
       " {'label': 'WEA', 'pattern': 'Tornado'},\n",
       " {'label': 'WEA', 'pattern': 'Orkan'},\n",
       " {'label': 'WEA', 'pattern': 'Wirbelwind'},\n",
       " {'label': 'WEA', 'pattern': 'Wirbelsturm'},\n",
       " {'label': 'WEA', 'pattern': 'Unwetter'},\n",
       " {'label': 'WEA', 'pattern': 'Taifun'},\n",
       " {'label': 'WEA', 'pattern': 'Südwind'},\n",
       " {'label': 'WEA', 'pattern': 'Südwestwind'},\n",
       " {'label': 'WEA', 'pattern': 'Westwind'},\n",
       " {'label': 'WEA', 'pattern': 'Nordwestwind'},\n",
       " {'label': 'WEA', 'pattern': 'Nordwind'},\n",
       " {'label': 'WEA', 'pattern': 'Nordostwind'},\n",
       " {'label': 'WEA', 'pattern': 'Ostwind'},\n",
       " {'label': 'WEA', 'pattern': 'Südostwind'},\n",
       " {'label': 'WEA', 'pattern': [{'LOWER': 'S.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA', 'pattern': [{'LOWER': 'S.W.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA',\n",
       "  'pattern': [{'LOWER': 'S.'}, {'LOWER': 'W.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA', 'pattern': [{'LOWER': 'W.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA', 'pattern': [{'LOWER': 'N.W.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA',\n",
       "  'pattern': [{'LOWER': 'N.'}, {'LOWER': 'W.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA', 'pattern': [{'LOWER': 'N.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA', 'pattern': [{'LOWER': 'N.O.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA',\n",
       "  'pattern': [{'LOWER': 'N.'}, {'LOWER': 'O.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA', 'pattern': [{'LOWER': 'O.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA', 'pattern': [{'LOWER': 'S.O.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA',\n",
       "  'pattern': [{'LOWER': 'S.'}, {'LOWER': 'O.'}, {'LOWER': 'Wind'}]},\n",
       " {'label': 'WEA', 'pattern': 'Flut'},\n",
       " {'label': 'WEA', 'pattern': 'Sutrmflut'},\n",
       " {'label': 'WEA', 'pattern': 'Hochwasser'},\n",
       " {'label': 'WEA', 'pattern': 'Hochflut'},\n",
       " {'label': 'WEA', 'pattern': 'Überschwemmung'},\n",
       " {'label': 'WEA', 'pattern': 'Überschweummungs'},\n",
       " {'label': 'WEA', 'pattern': 'Überschwemmungen'},\n",
       " {'label': 'WEA', 'pattern': 'Überflutung'},\n",
       " {'label': 'WEA', 'pattern': 'Überflutungen'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "engaging-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_md')\n",
    "nlp.disable_pipe('ner')\n",
    "nlp.add_pipe('entity_ruler')\n",
    "ruler = nlp.get_pipe('entity_ruler')\n",
    "ruler.add_patterns(patterns)\n",
    "ruler.to_disk('../data/models/entity_ruler_121021')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61a910",
   "metadata": {},
   "source": [
    "## First annotation batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coated-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pipeline/weather_events.json', 'r', encoding='utf8') as f:\n",
    "    dico = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "sporting-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df\n",
    "\n",
    "df = climdist.data.load('main', readability=True, heading2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "united-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_annotation_data(df, n=100, max_len=5000):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    while len(results) < n:\n",
    "        \n",
    "        sample = df[(df.readability==True) & (df.text_len < max_len)].sample(n)\n",
    "        \n",
    "        for i in tqdm(sample.index):\n",
    "            doc = nlp(df.loc[i, 'full_text'])\n",
    "            if len(doc.ents) > 0:\n",
    "                results.append(i)\n",
    "                \n",
    "        for i in results:\n",
    "            if results.count(i) > 1:\n",
    "                results.pop(i)\n",
    "                \n",
    "        print(f'{len(results)} results so far')\n",
    "                \n",
    "    results = results[:n+1]\n",
    "                \n",
    "    print('Finished')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "known-musician",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.61it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 14.04it/s]\n",
      "  2%|█▋                                                                                | 2/100 [00:00<00:05, 18.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.25it/s]\n",
      "  3%|██▍                                                                               | 3/100 [00:00<00:03, 29.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.75it/s]\n",
      "  1%|▊                                                                                 | 1/100 [00:00<00:10,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.94it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.40it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.43it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.96it/s]\n",
      "  2%|█▋                                                                                | 2/100 [00:00<00:05, 18.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.49it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.32it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.27it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.82it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.67it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.25it/s]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 results so far\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "choose_annotation_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "behind-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../pipeline/ner/annotation_batch_index_121021.json') as f:\n",
    "    ix = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "specific-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the texts corepsonding to indices\n",
    "annotation_selection = df.iloc[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "minor-liverpool",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\krister\\climdist\\v_env\\lib\\site-packages\\spacy\\util.py:758: UserWarning: [W095] Model 'de_core_news_md' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# grab the old model to assign entities as best as possible before starting annotation\n",
    "import spacy\n",
    "nlp = spacy.load('../data/archive/models/spacy_model_250421/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "binding-actor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created!\n"
     ]
    }
   ],
   "source": [
    "# convert to doccano format\n",
    "dt = Transformer()\n",
    "dt.pandas_to_doccano(annotation_selection, nlp, '../pipeline/ner/annotation_batch_131021.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8438f59",
   "metadata": {},
   "source": [
    "## Second annotation batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc545a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 16.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 results so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 results so far\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### create second annotation batch (13.11.21)\n",
    "\n",
    "new_batch_index = choose_annotation_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2de9c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the old index to check against duplicate entries\n",
    "with open('../pipeline/ner/annotation_batch_index_121021.json', 'r', encoding='utf8') as f:\n",
    "    old_index = json.load(f)\n",
    "    \n",
    "for ix in new_batch_index:\n",
    "    if ix in old_index:\n",
    "        print(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d3fa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new index\n",
    "with open('../pipeline/ner/annotation_batch_index_131121.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(new_batch_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8476144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new selection with the index\n",
    "new_annotation_selection = df.iloc[new_batch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "347485cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now grab the old model trained after the previous batch\n",
    "nlp = spacy.load('../data/models/main_2/model-best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e06d6ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created!\n"
     ]
    }
   ],
   "source": [
    "# convert to doccano format\n",
    "dt = Transformer()\n",
    "dt.pandas_to_doccano(new_annotation_selection, nlp, '../pipeline/ner/annotation_batch_131121.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63272390",
   "metadata": {},
   "source": [
    "## Prepare annotated data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "roman-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = []\n",
    "\n",
    "with open('../pipeline/ner/annotated_data_151121.jsonl', 'r', encoding='utf8') as f:\n",
    "    for line in list(f):\n",
    "        annotated_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bdba5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in annotated_data:\n",
    "    if len(entry['annotations']) == 0:\n",
    "        print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c2da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "endless-bradford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konstantinopel\n",
      "Tschatirdagh.\n",
      "Specify the label for this type of entity: LOC\n",
      "\n",
      "\n",
      "Allah Burgen\n",
      "Specify the label for this type of entity: PER\n",
      "\n",
      "\n",
      "Sturm\n",
      "Schnee\n",
      "Donner\n",
      "Specify the label for this type of entity: WEA\n",
      "\n",
      "\n",
      "И- August\n",
      "vom\n",
      "30. August\n",
      "vom 31. Juli\n",
      "Specify the label for this type of entity: DAT\n",
      "\n",
      "\n",
      "{24: 'LOC', 28: 'PER', 29: 'WEA', 23: 'DAT'}\n",
      "Doccano data imported\n"
     ]
    }
   ],
   "source": [
    "dt = Transformer()\n",
    "TRAIN_DATA = dt.doccano_to_spacy('../pipeline/ner/annotated_data_151121.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5ea6dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "997ed9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = trim_entity_spans(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "23e2b252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA == cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "321bffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "def convert_to_spacy3x(TRAIN_DATA, output_path):\n",
    "    \n",
    "    nlp = spacy.blank(\"de\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in TRAIN_DATA: # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(text[start:end])\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        if len(ents) == 0:\n",
    "            print(text)\n",
    "        db.add(doc)\n",
    "\n",
    "    db.to_disk(output_path) # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "74f877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and dev sets (85/15)\n",
    "\n",
    "np.random.shuffle(TRAIN_DATA) # shuffle the data in-place\n",
    "train_set = TRAIN_DATA[0:338]\n",
    "valid_set = TRAIN_DATA[338:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f3b7bdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lincoln'S\n",
      "Skipping entity\n",
      "Wohlershof\n",
      "Skipping entity\n",
      "Praaa\n",
      "Skipping entity\n",
      "Schnee\n",
      "Skipping entity\n",
      "Alexander\n",
      "Skipping entity\n",
      "Witterung\n",
      "Skipping entity\n",
      "OeselS\n",
      "Skipping entity\n",
      "Baden\n",
      "Skipping entity\n",
      "W3!W\n",
      "Skipping entity\n",
      "Riga\n",
      "Skipping entity\n",
      "Mikau\n",
      "Skipping entity\n",
      "Warschan\n",
      "Skipping entity\n",
      "Coblenz\n",
      "Skipping entity\n",
      "Schneegestöber\n",
      "Skipping entity\n",
      "Spanien\n",
      "Skipping entity\n",
      "Sübfrankretch\n",
      "Skipping entity\n",
      "Wilna\n",
      "Skipping entity\n",
      "Mailand\n",
      "Skipping entity\n",
      "Zrankfurt\n",
      "Skipping entity\n",
      "Wisby\n",
      "Skipping entity\n",
      "Carysto\n",
      "Skipping entity\n",
      "Mitau\n",
      "Skipping entity\n",
      "Sturm\n",
      "Skipping entity\n",
      "Paris\n",
      "Skipping entity\n",
      "Doßingen\n",
      "Skipping entity\n",
      "Süd-Karolma\n",
      "Skipping entity\n",
      "Vacca\n",
      "Skipping entity\n",
      "LübeS\n",
      "Skipping entity\n",
      "Skassy\n",
      "Skipping entity\n",
      "Diebitsch\n",
      "Skipping entity\n",
      "Goethe\n",
      "Skipping entity\n"
     ]
    }
   ],
   "source": [
    "convert_to_spacy3x(train_set, '../pipeline/ner/train.spacy')\n",
    "convert_to_spacy3x(valid_set, '../pipeline/ner/valid.spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "690c9f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_trailing_whitespace(TRAIN_DATA):\n",
    "    \n",
    "    for text in TRAIN_DATA:\n",
    "        for entity in text[1]['entities']:\n",
    "            entity_surface = text[0][entity[0]:entity[1]]\n",
    "            if entity_surface != entity_surface.strip() or entity_surface != entity_surface.lstrip():\n",
    "                print(text[0])\n",
    "                print('--------',entity_surface, '\\n')\n",
    "                \n",
    "find_trailing_whitespace(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2cefd4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18508/2835089461.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mfind_whitespace_in_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_DATA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18508/2835089461.py\u001b[0m in \u001b[0;36mfind_whitespace_in_entities\u001b[1;34m(TRAIN_DATA)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mentity_surface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mwhitespace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity_surface\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mwhitespace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity_surface\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mentity_surface\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "def find_whitespace_in_entities(TRAIN_DATA):\n",
    "    \n",
    "    whitespace = re.compile(r'\\s')\n",
    "    ct = 0\n",
    "    \n",
    "    for text in TRAIN_DATA:\n",
    "        print(ct)\n",
    "        for entity in text[1]['entities']:\n",
    "            entity_surface = text[0][entity[0]:entity[1]]\n",
    "            if whitespace.match(entity_surface[0]) or whitespace.match(entity_surface[-1]):\n",
    "                print(text[0])\n",
    "                print('--------',entity_surface, '\\n')\n",
    "        ct += 1\n",
    "                \n",
    "find_whitespace_in_entities(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7c60714c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lesefrüchte.\\n\\nKleinmiithige Seelen lassen in, Sturm des Lebens daS Steuer aus den\\nHänden fahren und geben ihr Heil den Launen von Wind und Wellen preis,\\noder erwarten zu ihrer Rettung den Eingriff einer höheren Macht. Der Mu\\nthige steuert mit fester Hand durch die Brandung hindurch , um auf dein Riff\\nmit dem Leben zugleich die Kraft zu neuen Kämpfen zu retten. D. B.',\n",
       " {'entities': [[0, 18, 'LOC'],\n",
       "   [1265, 1276, 'PER'],\n",
       "   [21, 25, 'LOC'],\n",
       "   [56, 60, 'LOC'],\n",
       "   [234, 251, 'PER'],\n",
       "   [1055, 1066, 'LOC'],\n",
       "   [1225, 1240, 'PER'],\n",
       "   [1248, 1263, 'PER'],\n",
       "   [1282, 1292, 'PER'],\n",
       "   [1314, 1334, 'PER'],\n",
       "   [1336, 1346, 'PER'],\n",
       "   [1348, 1355, 'PER'],\n",
       "   [1360, 1370, 'PER']]}]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda55f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0802f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55822932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63119253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "passing-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_train_data(data):\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    for old_entry in data:\n",
    "        new_entry = []\n",
    "        new_entry.append(old_entry['text'])\n",
    "        new_entry.append({})\n",
    "        new_entry[1]['entities'] = old_entry['entities']\n",
    "        new_data.append(new_entry)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "peripheral-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "corresponding-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_results():\n",
    "    i = random.choice(results)\n",
    "    doc = nlp(df.loc[i, 'full_text'])\n",
    "    print(df.loc[i, 'text_len'])\n",
    "    displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "detailed-retrieval",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Neueste Nachrichten</br></br></br>\tCharkow, 30. Januar. Mit Beziehung aui da« neuerdings colportirte Gerücht, wonach Osman Pascha vor ein Kriegsgericht gestellt werden solle, wird dem .GoloS&quot; von hier Folgendes telegraphirt: Soeben war ich bei OSman Pascha, welcher selbst von den gegen ihn erhobenen Beschuldigungen. als habe er 150 russische Gefangene lebend verscharren lassen, zu sprechen anfing. Er versichert, daß überhaupt nur 30 russische Gefangene bei ihm gewesen, welche sämmtlich  Sofia befördert worden seien. Bei dem Sturm vom 30. August find nach OSman'S Worten türkischerseitS überhaupt leine Gesangene gemacht worden.</br>Wien, 11. Febr. (30. Jan.) Von bestint'rrichter Seite werden alle Nochrichten über angeblich beabfich« tigte Mobilifirung österreichischer Truppen als völlig unbegründet bezeichnet ; ebenso ist die von auswärtigen Blättern gebrachte Nachricht, daß bereits Ordre zur Absendung eines österreichischen Geschwaders nach Konpantinopel ertheilt sei, unrichtig und dürfte diese Nach, rieht wohl aus die bereits vor mehreren Tagen gemel. dete Ausrüstung zweier Kriegsschiffe in Pola zurückzu« führen sein. (W. T.-B.)</br>Rom, 11. Februar (30. Januar). Die neuesten Nachrichten über den Beschluß hinsichtlich des Ortes deS Conclave lauten widersprechend. Dem .Diritto' zufolge bestätigt fich die Nachricht, daß bereitS beschlossen sei, das Conclave in Rom abzuhalten, nicht. (W. T..8.)</br>Paris, 10. Februar (29. Januar). Die von auswältigen Blättern gebrachte Nachricht, daß zwei sran« zösische Panzerschiffe Befehl erhalten hätten, nach Konstantinopel zu gehen, wird von der .Agence HavaS&quot; für unbegründet erklärt. (W. T.-B.)</br>Konstantinopel, 8. Februar (27. Jan.). Gemäß den Bestimmungen des Waffenstillstandes räumten die</br></br>\tTürken heute Widdin. Rustschuk. Silistria. Belgradschik und Erzerum. Die Truppen auS Rasgrad und die Behörden von Rustschuk zogen sich nach Schumla zurück. Ebenso haben die Türken die Denfenfivlinien von Konstantinopel, von Bujukstchckmedze und Hastemkoler bis DerfoS geräumt, da diese Positionen in die 12 Meilen breite neutrale Zone fallen, welche von Kuschuktschemedje bis Akbunar reicht. Die Russen halten Tschataldja besetzt; fie sichern ihre Verpflegung über Bourgas und Midia. Die Eröffnung der Schifffahrt auf der Donau wird demnächst erwartet. Die Pfote notificirte den Botschaftern der fremden Mächte die Aushebung der Blockade im Schwarzen Meere. Französische Packetboote werden unverweilt die Fahrten nach Konstantinopel und Odessa ausnehmen. — Zwei höhere russische Offiziere sind gestern hier eingetroffen und vom Sultan empfangen worden. Dieselben gehen heute nach Montenegro, um den zwischen den montenegrinischen und türkischen Offizieren zu führenden Verhandlungen bezüglich der Demarkationslinie beizuwohnen. Die Serben werden Waffenstillstandsbestimmungen zufolge USkup besetzen. (©. T.-B )</br>Konstantinopel, 8. Februar (27. lanuar). Die türkischen Bevollmächtigten Server Pascha und Namyk Pascha sind heute von Adrianopel hier eingetroffen. Server Pascha hat seine Dimission eingereicht. Wie verlautet, soll Savfet Pascha mit anderen Delegirten nach Adrianopel gehen zur Führung der VerHandlungen über den Präliminarfrieden und über den definitiven Friedensvertrag. Der Sultan hat den Großfürsten Nikolai eingeladen, einige Tage in Konstantinopel zu verweilen. Die Kammer hat die Regierung aufgefordert, Maßregeln gegen die Tscherkessen zu ergreifen, welche in Konstantinopel geraubte Gegenstande verkaufen. Die kriegsrechtliche Untersuchung gegen Mehmed Ali Pascha hat begonnen. (W. T.-B.)</br>Konstantinopel, 9. Februar (28. lanuar). Da in Folge des zwischen der Türkei und Rußland abgeschlossenen Waffenstillstandes die Feindseligkeiten sowohl zu Lande als auch zu Wasser eingestellt worden find, wird soeben durch einen kaiserlichen Irade die Aufhebung derßlockade desSchwarzen Meeres während der Dauer des Waffenstillstandes angeordnet. – (W. T.-B.)</br>Konstantinopel, 9. Februar (28. Januar). Im türkischen Parlament brachte der griechische Patriarch heftige Klagen über die Metzeleien vor, welche in 13 in der Nähe von Konstantinopel gelegenen Dörfern von Tscherkessen begangen worden find. (W.T.-B.)</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-confusion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "incredible-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_year(start):\n",
    "    \n",
    "    index = start\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        current = ix[index]\n",
    "        print(index)\n",
    "        print(df.loc[current, 'year'])\n",
    "        print('\\n')\n",
    "        print(df.loc[current, 'full_text'][:100])\n",
    "        cont = input('Next:')\n",
    "        index += 1\n",
    "        print('\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
