{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "yourprediction = nlp(yourtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "usual-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this one, you probably have to replace gold['entities'] with gold['labels'] (your data had diff. structure)\n",
    "\n",
    "def evaluate_ner(pred, gold):\n",
    "    \n",
    "    '''Inputs are a Spacy NLP object and a gold standard annotation.\n",
    "    Returns two a tuple of two lists of booleans, first for gold standards, second for predictions.\n",
    "    Lists can later be compared with Scikit-learn.confusion_matrix to get TP, FP, TN, FN scores.'''\n",
    "    \n",
    "    # dico for converting NE types from integer to string (ex. 385 to 'LOC')\n",
    "    types_labels = {0:0}\n",
    "    for ent in pred.ents:\n",
    "        types_labels[ent.label] = ent.label_\n",
    "        \n",
    "    # the spacy prediction is tokenised, unlike the gold standard annotation.\n",
    "    pred_tokens = []\n",
    "    pred_entities = []\n",
    "    pred_positions = []\n",
    "    \n",
    "    # loop over these tokens to get their positions in the doc and their entity types\n",
    "    for token in pred:\n",
    "        pred_tokens.append(token)\n",
    "        pred_entities.append(types_labels[token.ent_type])\n",
    "        pred_positions.append((token.idx,token.idx + len(token)))\n",
    "        \n",
    "    #print(list(zip(pred_tokens,pred_entities,pred_positions)))\n",
    "\n",
    "    # slice the gold standard text using the token positions from the prediction\n",
    "    gold_tokens = []  \n",
    "    for pos in pred_positions:\n",
    "        gold_tokens.append(gold['text'][pos[0]:pos[1]])\n",
    "    \n",
    "    # create a set of all unique entity categories in the gold standard\n",
    "    entity_set = []\n",
    "    for ent in gold['entities']:\n",
    "        if ent[2] not in entity_set:\n",
    "            entity_set.append(ent[2])\n",
    "     \n",
    "    # this one is a bit tricky. it creates a dico from the gold standardthat has all the character positions\n",
    "    # that contain a given entity type (try printing if makes no sense). It basically maps the areas of the\n",
    "    # gold standard text where each entity is present\n",
    "    gold_entity_ranges = {}\n",
    "    \n",
    "    for ent in entity_set:\n",
    "        entpos = []\n",
    "        for entity in gold['entities']:\n",
    "            if entity[2] == ent:\n",
    "                entpos += (list(range(entity[0], entity[1])))\n",
    "        gold_entity_ranges[ent] = entpos\n",
    "            \n",
    "            \n",
    "    # this creates a list for all of the tokens in the prediction. if the token is not in the range of\n",
    "    # any entity (cf last variable, gold_entity_ranges), the loop appends the label of the token, otherwise,\n",
    "    # it appends 0 (again, try printing)\n",
    "    gold_entities = []\n",
    "    \n",
    "    for pos in pred_positions:\n",
    "        isentity = False\n",
    "        for label in entity_set:\n",
    "            if set(range(pos[0], pos[1])) & set(gold_entity_ranges[label]):\n",
    "                isentity = True\n",
    "                gold_entities.append(label)\n",
    "                break\n",
    "        if not isentity:\n",
    "            gold_entities.append(0)\n",
    "            \n",
    "    #print(gold_entities)\n",
    "        \n",
    "        \n",
    "    # now finally we can create two boolean lists that describe the gold standards and the predictions\n",
    "    # on a token level for all the entities. for each label, there will be a boolean list \"label_gold\"\n",
    "    # that has the length of tokens in the doc. for each token, the list has 1 if the entity in question is\n",
    "    # present and 0 otherwise.\n",
    "    # the second list 'label_pred' does the same, but this time with the prediciton. the point is to make\n",
    "    # the predictions comparable for each label: if the two lists, label_gold and label_pred are identical\n",
    "    # for a label, your model got all the entities right in that category. if label_gold has more, your model\n",
    "    # missed some, and vice-versa\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for label in entity_set:\n",
    "        label_gold = [1 if ent==label else 0 for ent in gold_entities]\n",
    "        label_pred = [1 if ent==label else 0 for ent in pred_entities]\n",
    "        \n",
    "        results[label] = (label_gold, label_pred)\n",
    "        \n",
    "    return results       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_results = evaluate_ner(yourprediction, yourannotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regulation-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(your_results[0], your_results[1])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = matrix.ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f_score = 2 / ((recall**-1) + (precision)**-1)\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f-score: {f_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gold = []\n",
    "total_pred = []\n",
    "\n",
    "for yourprediction, yourannotation in zip(allyourpredictions, allyourannotations) # get all your stuff into to lists!\n",
    "    \n",
    "    eval_scores = evaluate_ner(yourprediction, yourannotation)\n",
    "    \n",
    "    for label in eval_scores:\n",
    "        total_gold += eval_scores[label][0]\n",
    "        total_pred += eval_scores[label][1]\n",
    "        \n",
    "matrix = confusion_matrix(total_gold, total_pred)\n",
    "matrix\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
